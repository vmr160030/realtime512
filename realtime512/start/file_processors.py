import os
import json
import numpy as np
import yaml

from ..helpers.bandpass_filter import apply_bandpass_filter
from ..helpers.time_shifts import optimize_time_shift, apply_time_shifts
from ..helpers.high_activity_intervals import detect_high_activity_intervals
from ..helpers.channel_spike_stats import compute_channel_spike_stats, detect_spikes_single_channel
from ..helpers.data_utils import set_high_activity_to_zero
from ..helpers.template_computation import compute_templates_from_frames
from ..helpers.generate_preview import generate_preview

def process_filtering(bin_files, raw_dir, computed_dir, n_channels, filter_params, sampling_frequency):
    """Apply bandpass filtering to raw .bin files."""
    for fname in bin_files:
        if not os.path.exists(os.path.join(computed_dir, "filt")):
            os.makedirs(os.path.join(computed_dir, "filt"))
        filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
        if os.path.exists(filt_path):
            # Check that it has the expected size (same size as raw file)
            raw_path = os.path.join(raw_dir, fname)
            raw_size = os.path.getsize(raw_path)
            filt_size = os.path.getsize(filt_path)
            if filt_size == raw_size:
                continue  # Already processed
            else:
                proceed = input(
                    f"Filtered file {filt_path} exists but size mismatch. Reprocess? (y/n): "
                )
                if proceed.lower() != 'y':
                    continue  # Skip reprocessing
                else:
                    # remove the existing file to reprocess
                    os.remove(filt_path)
        if not os.path.exists(filt_path):
            raw_path = os.path.join(raw_dir, fname)
            filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
            print(f"Filtering {fname}...")
            apply_bandpass_filter(
                input_path=raw_path,
                output_path=filt_path,
                num_channels=n_channels,
                lowcust=filter_params['lowcut'],
                highcut=filter_params['highcut'],
                fs=sampling_frequency,
                order=filter_params['order']
            )
            return True
    return False

def estimate_shift_coefficients(bin_files, computed_dir, electrode_coords, sampling_frequency):
    """Estimate shift coefficients from first filtered file."""
    shift_coeffs_path = os.path.join(computed_dir, "shift_coeffs.yaml")
    if os.path.exists(shift_coeffs_path):
        return
    if len(bin_files) == 0:
        return
    # path of first filtered file
    first_filt_path = os.path.join(computed_dir, "filt", bin_files[0] + ".filt")
    if not os.path.exists(first_filt_path):
        return
    
    c_x, c_y = optimize_time_shift(
        filtered_data_file_path=first_filt_path,
        electrode_coords=electrode_coords,
        sampling_frequency_hz=sampling_frequency,
        duration_sec=0.5
    )

    # write yaml file
    shift_data = {
        'c_x': c_x,
        'c_y': c_y
    }
    with open(shift_coeffs_path, "w") as f:
        yaml.dump(shift_data, f)
    print(f'Using shift coefficients: c_x={c_x:.6e}, c_y={c_y:.6e}')

def load_shift_coefficients(computed_dir):
    """Load shift coefficients from file if they exist."""
    shift_coeffs_path = os.path.join(computed_dir, "shift_coeffs.yaml")
    if os.path.exists(shift_coeffs_path):
        with open(shift_coeffs_path, "r") as f:
            shift_coeffs = yaml.safe_load(f)
        return shift_coeffs['c_x'], shift_coeffs['c_y']
    return None, None

def process_high_activity_intervals(bin_files, computed_dir, n_channels, sampling_frequency, high_activity_threshold):
    """Compute high activity intervals for filtered files."""
    for fname in bin_files:
        filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
        if not os.path.exists(os.path.join(computed_dir, "high_activity")):
            os.makedirs(os.path.join(computed_dir, "high_activity"))
        high_activity_path = os.path.join(computed_dir, "high_activity", fname + ".high_activity.json")
        if os.path.exists(high_activity_path):
            continue  # Already processed
        if not os.path.exists(filt_path):
            continue  # Filtered file does not exist yet
        print(f"Computing high activity intervals: {fname}.high_activity.json")
        high_activity_intervals = detect_high_activity_intervals(
            data_path=filt_path,
            num_channels=n_channels,
            num_frames=os.path.getsize(filt_path) // (2 * n_channels),
            sampling_frequency_hz=sampling_frequency,
            high_activity_threshold=high_activity_threshold
        )
        num_intervals = len(high_activity_intervals)
        print(f"  Found {num_intervals} high activity intervals.")
        total_high_activity_duration_sec = sum(end - start for start, end in high_activity_intervals)
        total_duration_sec = os.path.getsize(filt_path) / (2 * n_channels * sampling_frequency)
        print(f"  Total high activity duration: {total_high_activity_duration_sec:.2f} sec out of {total_duration_sec:.2f} sec.")
        # write to json file
        with open(high_activity_path, "w") as f:
            json.dump({
                "high_activity_intervals": [
                    {"start_sec": start, "end_sec": end} for start, end in high_activity_intervals
                ]
            }, f)
        return True
    return False

def process_spike_stats(bin_files, computed_dir, n_channels, sampling_frequency, detect_threshold_for_spike_stats):
    """Compute spike statistics for filtered files."""
    for fname in bin_files:
        filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
        if not os.path.exists(os.path.join(computed_dir, "stats")):
            os.makedirs(os.path.join(computed_dir, "stats"))
        stats_path = os.path.join(computed_dir, "stats", fname + ".stats.json")
        if os.path.exists(stats_path):
            continue  # Already processed
        if not os.path.exists(filt_path):
            continue  # Filtered file does not exist yet
        print(f"Computing channel spike stats: {fname}.stats.json")
        filt_data = np.fromfile(filt_path, dtype=np.int16).reshape(-1, n_channels)
        mean_firing_rates, mean_spike_amplitudes = compute_channel_spike_stats(
            data=filt_data,
            sampling_frequency_hz=sampling_frequency,
            threshold=detect_threshold_for_spike_stats
        )
        # write to json file
        with open(stats_path, "w") as f:
            json.dump({
                "mean_firing_rates": mean_firing_rates.tolist(),
                "mean_spike_amplitudes": mean_spike_amplitudes.tolist()
            }, f)
        return True
    return False

def process_time_shifts(bin_files, computed_dir, n_channels, sampling_frequency, c_x, c_y, electrode_coords):
    """Apply time shifts to filtered files."""
    if c_x is None or c_y is None:
        return
    for fname in bin_files:
        filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
        if not os.path.exists(os.path.join(computed_dir, "shifted")):
            os.makedirs(os.path.join(computed_dir, "shifted"))
        shifted_path = os.path.join(computed_dir, "shifted", fname + ".shifted")
        if os.path.exists(shifted_path):
            # Check that it has the expected size (same size as filt file)
            filt_size = os.path.getsize(filt_path)
            shifted_size = os.path.getsize(shifted_path)
            if shifted_size == filt_size:
                continue  # Already processed
            else:
                proceed = input(
                    f"Shifted file {shifted_path} exists but size mismatch. Reprocess? (y/n): "
                )
                if proceed.lower() != 'y':
                    continue  # Skip reprocessing
                else:
                    # remove the existing file to reprocess
                    os.remove(shifted_path)
        if not os.path.exists(shifted_path):
            print(f"Applying time shifts cx={c_x:.6e}, cy={c_y:.6e} to {fname}.filt...")
            filt_data = np.fromfile(filt_path, dtype=np.int16).reshape(-1, n_channels)
            shift_data = apply_time_shifts(
                data=filt_data,
                sampling_frequency_hz=sampling_frequency,
                c_x=c_x,
                c_y=c_y,
                electrode_coords=electrode_coords,
            )
            shift_data.astype(np.int16).tofile(shifted_path)
            print(f"Wrote shifted data to {shifted_path}.")
            return True
    return False

def process_templates(bin_files, computed_dir, n_channels, sampling_frequency, electrode_coords):
    """Compute templates for filtered files."""
    for fname in bin_files:
        shift_path = os.path.join(computed_dir, "shifted", fname + ".shifted")
        high_activity_path = os.path.join(computed_dir, "high_activity", fname + ".high_activity.json")
        if not os.path.exists(os.path.join(computed_dir, "templates")):
            os.makedirs(os.path.join(computed_dir, "templates"))
        templates_path = os.path.join(computed_dir, "templates", fname + ".templates.npy")
        if os.path.exists(templates_path):
            continue  # Already processed
        if not os.path.exists(shift_path):
            continue  # Shifted file does not exist yet
        if not os.path.exists(high_activity_path):
            continue  # High activity intervals do not exist yet
        print(f"Computing templates: {fname}.templates.npy")
        shift_data = np.fromfile(shift_path, dtype=np.int16).reshape(-1, n_channels)
        with open(high_activity_path, "r") as f:
            high_activity_data = json.load(f)
        high_activity_intervals = [
            (item['start_sec'], item['end_sec']) for item in high_activity_data['high_activity_intervals']
        ]
        shifted_data_low_activity = set_high_activity_to_zero(
            data=shift_data,
            sampling_frequency_hz=sampling_frequency,
            high_activity_intervals=high_activity_intervals,
            start_frame=0,
            end_frame=shift_data.shape[0]
        )
        data_min = np.min(shifted_data_low_activity, axis=1)
        detect_threshold = -80
        num_nearest_neighbors = 20
        num_clusters = 100
        spike_inds = detect_spikes_single_channel(
            data=data_min,
            threshold=detect_threshold,
            sign=-1,
            window_size=10
        )
        frames = shifted_data_low_activity[spike_inds, :].astype(np.float32)
        
        print(f'Computing templates from {frames.shape[0]} frames...')
        templates = compute_templates_from_frames(
            frames,
            num_nearest_neighbors=num_nearest_neighbors,
            num_clusters=num_clusters,
            electrode_coords=electrode_coords
        )
        np.save(templates_path, templates)
        return True
    return False

def process_preview(bin_files, computed_dir, n_channels, sampling_frequency, electrode_coords):
    for fname in bin_files:
        filt_path = os.path.join(computed_dir, "filt", fname + ".filt")
        shift_path = os.path.join(computed_dir, "shifted", fname + ".shifted")
        high_activity_intervals_path = os.path.join(computed_dir, "high_activity", fname + ".high_activity.json")
        stats_path = os.path.join(computed_dir, "stats", fname + ".stats.json")
        templates_path = os.path.join(computed_dir, "templates", fname + ".templates.npy")
        if not os.path.exists(os.path.join(computed_dir, "preview")):
            os.makedirs(os.path.join(computed_dir, "preview"))
        preview_path = os.path.join(computed_dir, "preview", fname + ".figpack")
        if os.path.exists(preview_path):
            continue  # Already processed
        if not os.path.exists(filt_path):
            continue  # Filtered file does not exist yet
        if not os.path.exists(shift_path):
            continue  # Shifted file does not exist yet
        if not os.path.exists(high_activity_intervals_path):
            continue  # High activity intervals do not exist yet
        if not os.path.exists(stats_path):
            continue  # Stats file does not exist yet
        if not os.path.exists(templates_path):
            continue  # Templates file does not exist yet
        with open(high_activity_intervals_path, "r") as f:
            high_activity_data = json.load(f)
        high_activity_intervals = [
            (item['start_sec'], item['end_sec']) for item in high_activity_data['high_activity_intervals']
        ]
        print(f"Generating preview figpack: {fname}.figpack")
        generate_preview(
            filt_path=filt_path,
            shift_path=shift_path,
            high_activity_intervals=high_activity_intervals,
            stats_path=stats_path,
            templates_path=templates_path,
            n_channels=n_channels,
            sampling_frequency=sampling_frequency,
            electrode_coords=electrode_coords,
            preview_path=preview_path
        )
        return True
    return False
